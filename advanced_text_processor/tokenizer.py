def tokenize(text):
    """
    Tokenizes text into a list of words.

    Args:
        text (str): Input text.

    Returns:
        list: List of tokens (words).
    """
    return text.split()
